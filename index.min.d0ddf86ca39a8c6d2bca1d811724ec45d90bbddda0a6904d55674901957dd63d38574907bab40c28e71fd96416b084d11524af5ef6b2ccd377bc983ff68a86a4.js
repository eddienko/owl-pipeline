var suggestions=document.getElementById('suggestions'),userinput=document.getElementById('userinput');document.addEventListener('keydown',inputFocus);function inputFocus(a){a.keyCode===191&&(a.preventDefault(),userinput.focus()),a.keyCode===27&&(userinput.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(b){const d=suggestions.querySelectorAll('a'),e=[...d],a=e.indexOf(document.activeElement);let c=0;b.keyCode===38?(b.preventDefault(),c=a>0?a-1:0,d[c].focus()):b.keyCode===40&&(b.preventDefault(),c=a+1<e.length?a+1:a,d[c].focus())}(function(){var b=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});b.add({id:0,href:"https://eddienko.github.io/owl-pipeline/docs/prologue/introduction/",title:"Introduction",description:"",content:'\u003cp\u003eThe Owl Scheduler is made from two different components.\nThe \u003ca href="/docs/server/install/"\u003eOwl Server\u003c/a\u003e is installed in the Kubernetes cluster and is in charge\nof receiving requests for jobs, allocating the necessary resources and running them. This consists of the scheduler itself\nand a lightway API that is used to communicate with the user.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href="/docs/client/install/"\u003eOwl Client\u003c/a\u003e runs in the user space\n(i.e. in any other system outside of the cluster) and is used to interact with the server,\nmainly to submit jobs and perform some administrative tasks.\u003c/p\u003e\n\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\n    \u003cdiv class="flex-shrink-1 alert-icon"\u003e\n        \u003cspan class="material-icons amber500"\u003einfo \u003c/span\u003e\n    \u003c/div\u003e\n    \n    \n    \u003cdiv class="w-100"\u003e We are releasing a preview version of the Owl Scheduler that has been in production for two years.\nWhile not all features have been released yet, we are working on bringing them in.\nThe current version is however usable and we welcome any input from the community in our\n\u003ca href="https://github.com/eddienko/owl-pipeline/discussions"\u003eDiscussions Forum\u003c/a\u003e.\u003c/div\u003e\n    \n    \n\u003c/div\u003e\n\u003ch2 id="pre-requisites"\u003ePre-Requisites\u003c/h2\u003e\n\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\n    \u003cdiv class="flex-shrink-1 alert-icon"\u003e\n        \u003cspan class="material-icons amber500"\u003einfo \u003c/span\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class="w-100"\u003eYou will need a running Kubernetes cluster. \u003c/div\u003e\n    \n\u003c/div\u003e\n\u003cp\u003eIn order to install and run the scheduler you need access to a Kubernetes cluster. We have tried Owl in the following\nversions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKubernetes 1.20 (Google Cloud Engine)\u003c/li\u003e\n\u003cli\u003eKubernetes 1.20 (On premises cloud)\u003c/li\u003e\n\u003cli\u003eKubernetes 1.21 (Docker Desktop)\u003c/li\u003e\n\u003cli\u003eKubernetes 1.22 (On premises cloud)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="get-started"\u003eGet started\u003c/h2\u003e\n\u003ch3 id="install-the-owl-server"\u003eInstall the Owl Server\u003c/h3\u003e\n\u003cp\u003eStep-by-step instructions on how to install the server in your Kubernetes cluster. \u003ca href="/docs/server/install/"\u003eOwl Server →\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id="install-the-owl-client"\u003eInstall the Owl Client\u003c/h3\u003e\n\u003cp\u003eStep-by-step instructions on how to install the client. \u003ca href="/docs/client/install/"\u003eOwl Client →\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id="tutorial"\u003eTutorial\u003c/h3\u003e\n\u003cp\u003eStep-by-step instructions on how to run your first job. \u003ca href="/docs/client/install/"\u003eTutorial →\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="contributing"\u003eContributing\u003c/h2\u003e\n\u003cp\u003eFind out how to contribute to the project. \u003ca href="/docs/help/how-to-contribute/"\u003eContributing →\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="help"\u003eHelp\u003c/h2\u003e\n\u003cp\u003eBrowse the Help section or head to the \u003ca href="https://github.com/eddienko/owl-pipeline/discussions"\u003eDiscussions Forum →\u003c/a\u003e\u003c/p\u003e\n'}).add({id:1,href:"https://eddienko.github.io/owl-pipeline/docs/prologue/architecture/",title:"Architecture",description:"",content:'\u003ch2 id="introduction"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eA job scheduler is an application that takes care of running unattended jobs in\na compute cluster. Typically the scheduler will add jobs to a queue and run them\nwhen resources become available. Some widely used schedulers (mainly in HPC\ncontext) are Portable Batch System (PBS), Slurm Workload Manager, Condor, Moab,\nand many others.\u003c/p\u003e\n\u003cp\u003eIn the context of cloud systems and in particular Kubernetes there is not a wide\nrange of applications to run user submitted batch jobs. For our use such an\napplication should be:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEasy to use.\u003c/strong\u003e Users should not be expected to know how the cluster is set up. A simple specification of how many CPUs and RAM should suffice in first instance with flexibility for more complex scheduling options.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRemote access.\u003c/strong\u003e Users should be able to submit, monitor and manage jobs from their own laptop or desktop without the need to login to a remote host.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDask support.\u003c/strong\u003e It should support Dask jobs, but not exclusively.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIntegration with Kubernetes.\u003c/strong\u003e The scheduler needs to create services and deployments in the K8s cluster, run jobs and, plug into external components (e.g. Django authentication) and interact with the database.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor this purpose we have built the Owl job scheduler from scratch. Owl is a\nframework to execute jobs in a compute cluster backed up by Kubernetes and Dask.\nIn essence Owl serves as very simple scheduler that accepts job descriptions,\nqueues them and submits them for execution keeping track of progress.\u003c/p\u003e\n\u003ch2 id="pipeline-definition-file"\u003ePipeline definition file\u003c/h2\u003e\n\u003cp\u003eWhen we refer to pipelines we understand parameterized jobs. The typical user chooses which pipeline to run from\nthose available in the system and submits a job with its own particular parameters. To do that the user\nonly needs to write a \u003cstrong\u003epipeline definition file (PDeF)\u003c/strong\u003e which includes the name of the pipeline to run,\nthe arguments needed to run it and a request for resources.\u003c/p\u003e\n\u003ch2 id="job-lifecycle"\u003eJob Lifecycle\u003c/h2\u003e\n\u003cp\u003eIn order to run a particular job (pipeline) users write a pipeline definition\nfile, in summary, a file that specifies the pipeline to run and the arguments required\nfor it to run as well as the compute resources required.\u003c/p\u003e\n\u003cp\u003eThis is submitted to the Owl API who logs the entry in a\ndatabase and places the job in a queue. Owl checks periodically for pipelines in\nthe queue and if they meet the scheduling requirements starts the allocation\nprocess. The main Owl process (a.k.a. Owl Scheduler) delegates the reponsibility\nof running the pipeline to a pipeline worker Pod process that is in charge of\ncontacting the K8s cluster, allocating the resources neeed and submitting the\npipeline to the compute cluster.\u003c/p\u003e\n\u003cp\u003eDepending on the pipeline requested resources, one or more containers running\nDask are allocated across the cluster and the pipeline starts execution. The\nWorker keeps communication with the running pipeline at all times. When\nfinished, or in case of error, informs Owl who retrieves the information from\nthe Worker, stops it and contacts the API for a database update.\u003c/p\u003e\n\u003cp\u003eThe steps followed when running a pipeline are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe user submits a pipeline definition file that is a YAML text file.\u003c/li\u003e\n\u003cli\u003eThe database is updated with the pipeline request. The scheduler queries the database when slots are available and allocates pipelines.\u003c/li\u003e\n\u003cli\u003eThe Owl scheduler starts a pipeline worker in a Pod sending the pipeline definition file and extra configuration needed.\u003c/li\u003e\n\u003cli\u003eThe pipeline worker loads the pipeline code from the pip repository and validates inputs against its schema.\u003c/li\u003e\n\u003cli\u003eThe Kubernetes cluster starts the requested number of Pod Dask workers.\u003c/li\u003e\n\u003cli\u003eThe pipeline worker starts a separate thread and runs the pipeline code. The code runs in the allocated containers using Dask taking advantage of parallelization over all workers.\u003c/li\u003e\n\u003cli\u003eThe main pipeline thread listens for heartbeat connections from the scheduler and waits for pipeline completion.\u003c/li\u003e\n\u003cli\u003eThe pipeline worker responds with the pipeline completion result to the scheduler.\u003c/li\u003e\n\u003cli\u003eThe scheduler stops the pipeline worker and logs an entry in the database.\u003c/li\u003e\n\u003c/ul\u003e\n'}).add({id:2,href:"https://eddienko.github.io/owl-pipeline/docs/server/install/",title:"Install",description:"",content:'\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\n    \u003cdiv class="flex-shrink-1 alert-icon"\u003e\n        \u003cspan class="material-icons amber500"\u003einfo \u003c/span\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class="w-100"\u003eThroughout this documentation we assume that you have a Kubernetes cluster already spin up and the control panel is reachable using command line tools. Helm 3 is used. \u003c/div\u003e\n    \n\u003c/div\u003e\n\u003ch2 id="add-the-helm-repository"\u003eAdd the Helm repository\u003c/h2\u003e\n\u003cp\u003eIf you have not done so, install \u003ca href="https://helm.sh/docs/intro/install/"\u003eHelm\u003c/a\u003e. Then add the repository\nthat contains the Owl chart:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ehelm repo add owl https://eddienko.github.io/owl-pipeline-server/\r\nhelm repo update\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="install-the-chart"\u003eInstall the chart\u003c/h2\u003e\n\u003cp\u003eThe Owl chart bootstraps the Owl Scheduler on Kubernetes using the Helm package manager.\nThe chart deploys the following components on the Kubernetes cluster:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOwl Scheduler\u003c/li\u003e\n\u003cli\u003eOwl API\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ehelm install owl owl/owl \\ \r\n      -n owl --create-namespace -f values.yaml\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis creates a namespace called \u003ccode\u003eowl\u003c/code\u003e if it does not exist and deploys all components there.\nHere \u003ccode\u003evalues.yaml\u003c/code\u003e is a YAML file containing local configuration details.\nA minimal example \u003ccode\u003evalues.yaml\u003c/code\u003e file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003e# Name of Docker image used to run the server\r\nimage:\r\n  repository: imaxt/owl-server\r\n  pullPolicy: IfNotPresent\r\n  tag: \u0026quot;0.6.1\u0026quot;\r\n\r\n# Create service account in RBAC clusters\r\nserviceAccount:\r\n  create: true\r\n\r\n# Secret token for communication between the Owl scheduler and the API\r\ntoken: \u0026quot;hdCI6MTYyOTI5NTczM30.tWSY38h7HfhtAnMkVJWf9gF-fU_EHCTB7zrar3QyLiA\u0026quot;\r\n\r\n# Global log level\r\nloglevel: DEBUG\r\n\r\n# Database DBI\r\ndbi: sqlite:////var/run/owl/sqlite.db\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMore complete examples can be found in the\n\u003ca href="https://github.com/eddienko/owl-pipeline-server/blob/main/examples"\u003erepository\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id="inspect-the-deployment-status"\u003eInspect the deployment status\u003c/h2\u003e\n\u003cp\u003eAfter deployment, you should have two pods running coallocated in the same node\n(in the default deployment both pods share the same PVC). The first pod is\nthe Owl API which is used to query the database and allow for external interaction,\ni.e. to submit and manage jobs. The second pod, the scheduler, queries the\nAPI for submitted jobs and schedules them in the cluster.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e$ kubectl get all --namespace owl\r\nNAME                                 READY   STATUS    RESTARTS   AGE\r\npod/owl-api-7cd474c655-9s887         1/1     Running   0          18m\r\npod/owl-scheduler-7996ff7f76-ktgfp   1/1     Running   0          22m\r\n\r\nNAME                        TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)             AGE\r\nservice/owl-api             LoadBalancer   10.4.15.167   34.88.233.217   8002:30923/TCP      3h36m\r\nservice/owl-scheduler       ClusterIP      10.4.7.4      \u0026lt;none\u0026gt;          7001/TCP,7002/TCP   3h36m\r\n\r\nNAME                            READY   UP-TO-DATE   AVAILABLE   AGE\r\ndeployment.apps/owl-api         1/1     1            1           95m\r\ndeployment.apps/owl-scheduler   1/1     1            1           95m\r\n\r\nNAME                                       DESIRED   CURRENT   READY   AGE\r\nreplicaset.apps/owl-api-7cd474c655         1         1         1       95m\r\nreplicaset.apps/owl-scheduler-7996ff7f76   1         1         1       55m\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA common reason that pods stay in \u003ccode\u003epending\u003c/code\u003e is the lack of resources. Make sure that at least one\nnode has resources to schedule both pods. In any case\n\u003ccode\u003ekubectl describe pod [name-of-the-pod] --namespace owl\u003c/code\u003e should give the cause.\u003c/p\u003e\n\u003ch2 id="expose-the-api-service"\u003eExpose the API service\u003c/h2\u003e\n\u003cp\u003eThe API service is exposed using a LoadBalancer. The external IP can be obtained either looking\nat the output from the command above or directly using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl get services --namespace owl \\\r\n -l \u0026quot;app.kubernetes.io/name=owl-api,app.kubernetes.io/instance=owl\u0026quot; \\\r\n -o jsonpath=\u0026quot;{.items[0].status.loadBalancer.ingress[0].ip}\u0026quot;\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe address of the API server is then \u003ccode\u003ehttp://34.88.233.217:8002\u003c/code\u003e. Exposing this to the outside\nworld can be also accomplished with a default ingress or using traefik. For reference this is a traefik\ndeployment to make the API available in a subpath\n\u003ca href="https://github.com/eddienko/owl-pipeline-server/blob/main/examples/imaxt/ingress.yaml"\u003eingress.yaml\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id="deployment-in-gke"\u003eDeployment in GKE\u003c/h2\u003e\n\u003cp\u003eIn Google Kubernetes Engine the cluster created with GKE Autopilot is not yet supported. Use a standard GKE cluster instead with (some) nodes using at least the \u003ccode\u003ee2-standard-4\u003c/code\u003e type.\u003c/p\u003e\n\u003cp\u003eAn example values file for deploying the Helm chart in GKE is available from\n\u003ca href="https://github.com/eddienko/owl-pipeline-server/blob/main/examples/gce/values.yaml"\u003evalues.yaml\u003c/a\u003e.\u003c/p\u003e\n'}).add({id:3,href:"https://eddienko.github.io/owl-pipeline/docs/server/configuration/",title:"Configuration",description:"",content:'\u003ch2 id="admin-password"\u003eAdmin password\u003c/h2\u003e\n\u003cp\u003eOn the first launch the database schema is created and a defaul random admin password created. You can recover the\ndefault password using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ePOD_NAME=$(kubectl get pods --namespace owl -l \u0026quot;app.kubernetes.io/name=owl-scheduler,app.kubernetes.io/instance=owl\u0026quot; -o jsonpath=\u0026quot;{.items[0].metadata.name}\u0026quot;)\r\nADMIN_PASSWORD=$(kubectl exec $POD_NAME -- cat /var/run/owl/adminPassword)\r\necho $ADMIN_PASSWORD\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="command-line-pod"\u003eCommand line Pod\u003c/h2\u003e\n\u003cp\u003eRun a Owl Client command line Pod in the server or install the Owl Client\nfor remote access (this assumes the API is available remotely). In the first case run the Pod:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl apply --namespace owl -f https://git.io/J0lsu\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand when ready start a command line prompt:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003ekubectl exec -it owlcli --namespace owl -- /bin/bash\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAn alternative is to install the \u003ca href="/docs/client/install/"\u003eOwl Client\u003c/a\u003e and define\nan environmental variable pointing to the location of the API:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eexport OWL_API_URL=https://api.owl.com\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="update-admin-password"\u003eUpdate admin password\u003c/h2\u003e\n\u003cp\u003eUse the password to login and change the admin password:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e# Use the initial password\r\nowl auth login\r\n\r\n# Update the password\r\nowl admin user update --admin admin yourNewPassword\r\n\r\n# Login with the new password\r\nowl auth login\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="add-pipeline-signatures"\u003eAdd pipeline signatures\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e# Example pipeline\r\ncurl -O http://somewhere.com/example.yaml\r\nowl admin pdef add example.yaml\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCheck that they available:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e# List all signatures\r\nowl pdef list\r\n\r\n# Get pipeline definition for example pipeline\r\nowl pdef get example\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="submit-your-first-pipeline"\u003eSubmit your first pipeline\u003c/h2\u003e\n\u003cp\u003eRetrieve the \u003ccode\u003eexample\u003c/code\u003e pipeline definition file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl pdef get example -o example.yaml\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eChange the parameters or resources and submit it with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl job submit example.yaml\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="query-pipeline-status"\u003eQuery pipeline status\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl job status get 1\r\n\u003c/code\u003e\u003c/pre\u003e\n'}).add({id:4,href:"https://eddienko.github.io/owl-pipeline/docs/server/storage/",title:"Storage",description:"",content:'\u003ch2 id="scheduler"\u003eScheduler\u003c/h2\u003e\n\u003cp\u003eBy default a PVC is created that stores the SQLite database and logs from the pipelines. This is mounted\nin \u003ccode\u003e/var/run/owl\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTBD.\u003c/p\u003e\n\u003ch2 id="pipeline-jobs"\u003ePipeline jobs\u003c/h2\u003e\n\u003cp\u003eTBD.\u003c/p\u003e\n'}).add({id:5,href:"https://eddienko.github.io/owl-pipeline/docs/server/advanced/",title:"Advanced",description:"",content:'\u003ch2 id="external-user-database"\u003eExternal user database\u003c/h2\u003e\n\u003cp\u003eTBD. It is possible to have the user database managed by an external application, e.g. a Django site.\u003c/p\u003e\n\u003ch2 id="custom-docker-image-for-pipelines"\u003eCustom Docker image for pipelines\u003c/h2\u003e\n\u003cp\u003eAs part of the \u003ccode\u003eresources\u003c/code\u003e section of the pipeline definition file it is possible\nto specify a custom Docker image, e.g.:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eresources:\r\n  threads: 10\r\n  workers: 2\r\n  memory: 10\r\n  image: myrepo/customimage:tag\r\n\u003c/code\u003e\u003c/pre\u003e\n'}).add({id:6,href:"https://eddienko.github.io/owl-pipeline/docs/server/troubleshoot/",title:"Troubleshooting",description:"",content:""}).add({id:7,href:"https://eddienko.github.io/owl-pipeline/docs/client/install/",title:"Quick Start",description:"",content:'\u003cp\u003eThw Owl client is used to interact with the Owl Server and submit jobs to the Kubernetes cluster.\nIt can also be used to perform some admin tasks.\u003c/p\u003e\n\u003ch2 id="install"\u003eInstall\u003c/h2\u003e\n\u003cp\u003eThe Owl client requires Python 3.7 or higher. It is recommended to create a Python environment,\ne.g., using conda:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003econda create -n pipelines python=3.7\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn order to use the new environment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003econda activate pipelines\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe latest stable version of the Owl client can be installed from PyPi:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003epip install owl-pipeline-client\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="authenticate-to-remote-server"\u003eAuthenticate to remote server\u003c/h2\u003e\n\u003cp\u003eUsing your credentials\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl auth login\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will ask for your username and password. Credentials will be saved to a file in your home\ndirectory \u003ccode\u003e$HOME/.owlrc\u003c/code\u003e.\u003c/p\u003e\n\u003ch2 id="pipeline-definition-file"\u003ePipeline definition file\u003c/h2\u003e\n\u003cp\u003eList all available pipelines in the remote server using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl pdef list\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf the server has been configured as per the\ninstructions the \u003ccode\u003eexample\u003c/code\u003e pipeline will be available.\nRetrieve the pipeline definition file using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl pdef get example -o example.yml\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis looks like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003e# Version of the configuration file\r\nversion: 1\r\n\r\n# Name of the pipeline\r\nname: example\r\n\r\n# Pipeline arguments\r\ndatalen: 100\r\n\r\n# Resources requested\r\nresources:\r\n  threads: 10\r\n  workers: 2\r\n  memory: 10\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe example pipeline runs a series of dummy computations using \u003ca href="https://dask.org"\u003eDask\u003c/a\u003e.\nIt has a unique input parameter\n\u003ccode\u003edatalen\u003c/code\u003e which basically controls how long the pipeline runs.\u003c/p\u003e\n\u003ch2 id="submit-pipeline"\u003eSubmit pipeline\u003c/h2\u003e\n\u003cp\u003eAdjust the resources and submit the pipeline using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl job submit example.yml\r\n\u003c/code\u003e\u003c/pre\u003e\n'}).add({id:8,href:"https://eddienko.github.io/owl-pipeline/docs/client/commands/",title:"User Commands",description:"",content:'\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\n    \u003cdiv class="flex-shrink-1 alert-icon"\u003e\n        \u003cspan class="material-icons amber500"\u003einfo \u003c/span\u003e\n    \u003c/div\u003e\n    \n    \n    \u003cdiv class="w-100"\u003e Make sure the \u003ccode\u003eOWL_API_URL\u003c/code\u003e environmental variable points to the public address of the Owl API service or\nadd the \u003ccode\u003e--api\u003c/code\u003e argument to all commands.\u003c/div\u003e\n    \n    \n\u003c/div\u003e\n\u003ch2 id="authentication"\u003eAuthentication\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eAuthenticate in the remote server\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl auth login\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eRemove authentication token from local and remote server.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl auth logout\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eChange password.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl auth change_password\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="pipeline-definitions"\u003ePipeline definitions\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eList available pipeline definitions in the system\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl pdef list\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eGet pipeline definition for pipeline \u003ccode\u003eexample\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003e# Display in the console\r\nowl pdef get example \r\n\r\n# Save to a file\r\nowl pdef get example -o example.yaml\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="job-submission-and-management"\u003eJob submission and management\u003c/h2\u003e\n\u003cp\u003eCommands to operate in jobs. Note that unless the user has admin privileges these\ncommands can only operate in the user\u0026rsquo;s own jobs.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSubmit a Job. The command wil return the unique ID of the job.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl job submit pipeline.yml\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eList jobs. This will list the jobs for the current user. If user has admin\nprivileges, the \u003ccode\u003e-all\u003c/code\u003e flag will return all jobs. The \u003ccode\u003e--max\u003c/code\u003e argument specifies\nthe maximum number of jobs to return and \u003ccode\u003e--latest\u003c/code\u003e sets the ordering. Pipelines\nin particular statuses can be retrieved with the \u003ccode\u003e--status\u003c/code\u003e flag.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl job list [--all] [--max 10] [--latest] [--status RUNNING]\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eQuery status of a job. If \u003ccode\u003e--json\u003c/code\u003e is given, a detailed description\nin JSON is returned.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl job status [--json] 1\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eCancel a job.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl job cancel 1\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eInspect logs from job.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl job logs [-f] 1\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eRerun a job (the job must be in status finished, error or cancelled).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl job rerun 1\r\n\u003c/code\u003e\u003c/pre\u003e\n'}).add({id:9,href:"https://eddienko.github.io/owl-pipeline/docs/client/admin/",title:"Admin Commands",description:"",content:'\u003cp\u003eSome of the commands in the previous section operate slightly different if the\nuser issuing them has admin privileges or not. E.g. an admin user will be able\nto cancel any running pipeline while normal users can only modify theirs.\u003c/p\u003e\n\u003cp\u003eThis section lists specific administrative commands.\u003c/p\u003e\n\u003ch2 id="users"\u003eUsers\u003c/h2\u003e\n\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\n    \u003cdiv class="flex-shrink-1 alert-icon"\u003e\n        \u003cspan class="material-icons amber500"\u003ewarning_amber \u003c/span\u003e\n    \u003c/div\u003e\n    \n    \n    \u003cdiv class="w-100"\u003e The \u003ccode\u003eadmin user\u003c/code\u003e commands should not be used if the user table is managed by other system,\ne.g. Django.\u003c/div\u003e\n    \n    \n\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eAdd user\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl admin user add [--admin] [--active] [--no-active] [-p password] username\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eUpdate user\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl admin user update [--admin] [--active] [--no-active] [-p password] username\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eDelete user\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl admin user delete username\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eList users\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl admin user list\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eList one user\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl admin user get username\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="pipeline-signatures"\u003ePipeline signatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eAdd or update a pipeline signature\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl admin pdef add pipedef.yaml\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eDelete a pipeline signature\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl admin pdef delete name\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="set-flags-in-the-server"\u003eSet flags in the server\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSet maintance mode on/off. When the setting is on queued pipelines will not be started.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl admin cmd \'{\u0026quot;maintenance\u0026quot;: \u0026quot;on\u0026quot;}\'\r\n\r\nowl admin cmd \'{\u0026quot;maintenance\u0026quot;: \u0026quot;off\u0026quot;}\'\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eSet maximum number of pipelines that can be in \u003ccode\u003eRUNNING\u003c/code\u003e state at the same time.\nSet to zero to disable.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl admin cmd \'{\u0026quot;maxpipe\u0026quot;: 4}\'\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eSet scheduler heartbeat. This is the time between heartbeat checks (available pipelines, pipelines status, cluster status, etc). Set to zero for default as defined in the configuration.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class="language-bash"\u003eowl admin cmd \'{\u0026quot;heartbeat\u0026quot;: 60}\'\r\n\u003c/code\u003e\u003c/pre\u003e\n'}).add({id:10,href:"https://eddienko.github.io/owl-pipeline/docs/pipelines/pipelines/",title:"Owl Pipelines",description:"",content:'\u003cp\u003eOwl pipelines are pip installable Python packages. In order to submit a pipeline\nto the system, the admin must have activated it and you need a\n\u003cstrong\u003epipeline definition file (PDeF)\u003c/strong\u003e. This is a YAML file that contains\nthe name of the pipeline, its arguments and requested resources.\u003c/p\u003e\n\u003cp\u003eCurrently there are three general purpose pipelines available.\u003c/p\u003e\n\u003ch2 id="example-pipeline"\u003eExample Pipeline\u003c/h2\u003e\n\u003cp\u003eThis is just a pipeline that uses Dask to run some dummy computations and\ncan be used for general testing and as a template for writing more\ncomplicated pipelines.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eversion: 1\r\n\r\n# Name of the pipeline\r\nname: example\r\n\r\n# Pipeline arguments\r\ndatalen: 100\r\n\r\n# Resources requested\r\nresources:\r\n  threads: 10\r\n  workers: 2\r\n  memory: 10\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="shell-pipeline"\u003eShell pipeline\u003c/h2\u003e\n\u003cp\u003eThe shell pipeline allows to execute an arbitrary command, typically a shell or a python script.\u003c/p\u003e\n\u003cp\u003eThe pipeline definition file is below (and can be obtained typing \u003ccode\u003eowl pdef get shell\u003c/code\u003e):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eversion: 1.2\r\n\r\n# Name of the pipeline\r\nname: shell\r\n\r\n# Directory where the command is writing data to (optional)\r\n# output_dir: /tmp/output\r\n\r\ncommand: [\u0026quot;sleep\u0026quot;, \u0026quot;300\u0026quot;]\r\n\r\n# use_dask: false\r\n\r\nresources:\r\n  workers: 1\r\n  memory: 8\r\n  threads: 1\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003eoutput_dir\u003c/code\u003e is optional and if specified the sheduler will save a log file\nand the configuration used to run the pipeline as well as a list of\nenvironmental variables.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ecommand\u003c/code\u003e parameter defines which command or script to run. The example above\njust waits for 5 minutes. Other command examples:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003e# Execute a Python script\r\ncommand: [\u0026quot;/opt/conda/bin/python\u0026quot;, \u0026quot;script.py\u0026quot;]\r\n\r\n# Execute a script that takes two arguments as input\r\ncommand: [\u0026quot;/home/eglez/scripts/script.sh\u0026quot;, \u0026quot;100\u0026quot;, \u0026quot;200\u0026quot;]\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that the path to the executable must be specified fully and the command is\na list containing all parts of the commands, i.e. the command ls -la would be\nwritten as [\u0026ldquo;ls\u0026rdquo;, \u0026ldquo;-la\u0026rdquo;].\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003euse_dask\u003c/code\u003e parameters needs a bit of explanation. In the default mode (false)\nthe script is run in only one worker with access to as much memory and cores\nrequested. Internally the (python) script can use Dask, multiprocessing,\nmultithreading or any other mechanism but the resources will be fixed to one\nworker.\u003c/p\u003e\n\u003cp\u003eIf use_dask is true then it makes sense to request more workers. It is required\nthe python script connects to the Dask scheduler as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-python"\u003eimport os\r\nfrom distributed import Client\r\n\r\nDASK_SCHEDULER = os.getenv(\u0026quot;DASK_SCHEDULER_ADDRESS\u0026quot;)\r\nclient = Client(DASK_SCHEDULER)\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="papermill-pipeline"\u003ePapermill pipeline\u003c/h2\u003e\n\u003cp\u003eThe papermill pipeline executes Jupyter notebooks using\n\u003ca href="https://papermill.readthedocs.io"\u003ePapermill\u003c/a\u003e. Plase read the official\ndocumentation on how to paramaterize the notebooks.\u003c/p\u003e\n\u003cp\u003eAn example pipeline definition file is below (and can be obtained typing \u003ccode\u003eowl\r pdef get papermill\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003eversion: 1.2\r\n\r\n# Name of the pipeline\r\nname: papermill\r\n\r\ninput_dir: /home/user/myDir\r\noutput_dir: /home/user/myDir\r\nnotebook: notebook.ipynb\r\n\r\nparamaters: {}\r\n\r\n# use_dask: false\r\n\r\nresources:\r\n  workers: 1\r\n  memory: 8\r\n  threads: 1\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe pipeline requires the input and output directories and the name of the\nnotebook to execute. The executed notebook will be saved in the output directory\nwith the name \u003ccode\u003enotebook_out.ipynb\u003c/code\u003e in this example.\u003c/p\u003e\n\u003cp\u003eThe parameters required to run the notebook are defined in parameters.\u003c/p\u003e\n\u003cp\u003eThe following shows an example output notebook that includes outputs and figures.\u003c/p\u003e\n\u003cfigure class="border-0"\u003e\r\n  \u003cimg class="img-fluid lazyload blur-up" data-sizes="auto" src="/docs/pipelines/pipelines/paperpipe_hu1b69d9c4d231fb584bc794f877d2a532_78177_20x0_resize_box_3.png" data-srcset="https://eddienko.github.io/owl-pipeline/docs/pipelines/pipelines/paperpipe_hu1b69d9c4d231fb584bc794f877d2a532_78177_800x0_resize_box_3.png 800w,https://eddienko.github.io/owl-pipeline/docs/pipelines/pipelines/paperpipe_hu1b69d9c4d231fb584bc794f877d2a532_78177_700x0_resize_box_3.png 700w,https://eddienko.github.io/owl-pipeline/docs/pipelines/pipelines/paperpipe_hu1b69d9c4d231fb584bc794f877d2a532_78177_600x0_resize_box_3.png 600w,https://eddienko.github.io/owl-pipeline/docs/pipelines/pipelines/paperpipe_hu1b69d9c4d231fb584bc794f877d2a532_78177_500x0_resize_box_3.png 500w" width="926" height="1107" alt="Papermill Pipeline"\u003e\r\n  \u003cnoscript\u003e\u003cimg class="img-fluid" sizes="100vw" srcset="https://eddienko.github.io/owl-pipeline/docs/pipelines/pipelines/paperpipe_hu1b69d9c4d231fb584bc794f877d2a532_78177_800x0_resize_box_3.png 800w,https://eddienko.github.io/owl-pipeline/docs/pipelines/pipelines/paperpipe_hu1b69d9c4d231fb584bc794f877d2a532_78177_700x0_resize_box_3.png 700w,https://eddienko.github.io/owl-pipeline/docs/pipelines/pipelines/paperpipe_hu1b69d9c4d231fb584bc794f877d2a532_78177_600x0_resize_box_3.png 600w,https://eddienko.github.io/owl-pipeline/docs/pipelines/pipelines/paperpipe_hu1b69d9c4d231fb584bc794f877d2a532_78177_500x0_resize_box_3.png 500w" src="/docs/pipelines/pipelines/paperpipe.png" width="926" height="1107" alt="Papermill Pipeline"\u003e\u003c/noscript\u003e\r\n  \u003cfigcaption class="figure-caption"\u003e\u003cem\u003ePapermill Pipeline\u003c/em\u003e\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n'}).add({id:11,href:"https://eddienko.github.io/owl-pipeline/docs/pipelines/signatures/",title:"Pipeline Signatures",description:"",content:'\u003cp\u003ePipelines are Python packages installable using \u003ccode\u003epip\u003c/code\u003e, i.e. they can uploaded to PyPi or\nto a private repository. In order for the server to run a pipeline this needs to be\ndefined as active in the system. Otherwise any pip installable package with the correct\nentrypoint could be run in the system posing a security risk. For this we\ndefine pipeline definition signatures.\u003c/p\u003e\n\u003cp\u003eThe pipeline definition signatures that are used with the \u003ccode\u003eadmin pdef\u003c/code\u003e commands\n(see \u003ca href="/docs/client/admin/#pipeline-definitions"\u003eAdmin commands\u003c/a\u003e)\nare slightly different to the pipeline definition files used to submit a pipeline.\u003c/p\u003e\n\u003cp\u003eThe difference is the pressence of two additional keywords in the former: \u003ccode\u003eextra_pip_packages\u003c/code\u003e and \u003ccode\u003eactive\u003c/code\u003e prepended with \u003ccode\u003esig:\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-yaml"\u003e# Version of the configuration file\r\nversion: 1\r\n\r\nsig:extra_pip_packages: owl-example-pipeline\r\nsig:active: True\r\n\r\n# Name of the pipeline\r\nname: example\r\n\r\n# Pipeline arguments\r\ndatalen: 100\r\n\r\n# Resources requested\r\nresources:\r\n  threads: 10\r\n  workers: 2\r\n  memory: 2\r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eextra_pip_packages\u003c/strong\u003e lists the package that contains the pipeline\nand \u003cstrong\u003eactive\u003c/strong\u003e sets if users can use it to run pipelines in the server.\u003c/p\u003e\n\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\n    \u003cdiv class="flex-shrink-1 alert-icon"\u003e\n        \u003cspan class="material-icons amber500"\u003einfo \u003c/span\u003e\n    \u003c/div\u003e\n    \n    \n    \u003cdiv class="w-100"\u003e At the moment it is not possible to have two versions of the same pipeline signature.\u003c/div\u003e\n    \n    \n\u003c/div\u003e\n'}).add({id:12,href:"https://eddienko.github.io/owl-pipeline/docs/pipelines/development/",title:"Creating Pipelines",description:"",content:"\u003cp\u003eOwl pipelines are pip installable Python packages. This page shows how to create a pipeline from scratch.\u003c/p\u003e\n"}).add({id:13,href:"https://eddienko.github.io/owl-pipeline/docs/help/how-to-contribute/",title:"How to Contribute",description:"",content:'\u003cdiv class="alert alert-warning d-flex" role="alert"\u003e\n    \u003cdiv class="flex-shrink-1 alert-icon"\u003e\n        \u003cspan class="material-icons amber500"\u003ehelp_outline \u003c/span\u003e\n    \u003c/div\u003e\n    \n    \n    \u003cdiv class="w-100"\u003e We welcome any comments on the usefulness of this software, ideas, enhancements, questions, etc.\u003c/div\u003e\n    \n    \n\u003c/div\u003e\n\u003ch3 id="join-the-discussion"\u003eJoin the discussion\u003c/h3\u003e\n\u003cp\u003eJoin the \u003ca href="https://github.com/eddienko/owl-pipeline-server/discussions"\u003ediscussion\u003c/a\u003e forum in GitHub\nand let us know if you have tried the software and/or any other feedback or question that you may have.\u003c/p\u003e\n\u003ch3 id="create-an-issue"\u003eCreate an issue\u003c/h3\u003e\n\u003cp\u003eIf you want to contribute to the code or propose new features or enhancements you can fill an issue\nin the corresponding GitHub issues section. These are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor the Owl Scheduler go to \u003ca href="https://github.com/eddienko/owl-pipeline-server/issues"\u003eOwl Scheduler Issues →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eFor the Owl Client go to \u003ca href="https://github.com/eddienko/owl-pipeline-client/issues"\u003eOwl Client Issues →\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eFor documentation go to \u003ca href="https://github.com/eddienko/owl-pipeline/issues"\u003eOwl Documentation Issues →\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n'}).add({id:14,href:"https://eddienko.github.io/owl-pipeline/docs/help/troubleshooting/",title:"Troubleshooting",description:"Solutions to common problems.",content:""}).add({id:15,href:"https://eddienko.github.io/owl-pipeline/docs/help/faq/",title:"FAQ",description:"Answers to frequently asked questions.",content:""}).add({id:16,href:"https://eddienko.github.io/owl-pipeline/docs/help/",title:"Help",description:"Help Doks.",content:""}).add({id:17,href:"https://eddienko.github.io/owl-pipeline/docs/client/",title:"Owl Client",description:"Owl Client",content:""}).add({id:18,href:"https://eddienko.github.io/owl-pipeline/docs/prologue/",title:"Owl Scheduler",description:"",content:""}).add({id:19,href:"https://eddienko.github.io/owl-pipeline/docs/",title:"Docs",description:"Docs Doks.",content:""}),userinput.addEventListener('input',c,!0),suggestions.addEventListener('click',e,!0);function c(){var h=this.value,f=b.search(h,{limit:5,index:["content"],enrich:!0}),g=suggestions.childNodes,e=0,i=f.length,c;for(suggestions.classList.remove('d-none'),f.forEach(function(b){c=document.createElement('div'),c.innerHTML='<a href><span></span><span></span></a>',a=c.querySelector('a'),t=c.querySelector('span:first-child'),d=c.querySelector('span:nth-child(2)'),a.href=b.result[e].doc.href,t.textContent=b.result[e].doc.title,d.textContent=b.result[e].doc.description,suggestions.appendChild(c)});g.length>i;)suggestions.removeChild(g[e])}function e(){while(suggestions.lastChild)suggestions.removeChild(suggestions.lastChild);return!1}})()